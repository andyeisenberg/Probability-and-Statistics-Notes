<section xml:id="sec-Conditional-Probability" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Conditional Probability</title>

  <subsection xml:id="subsec-conditional-probability">
    <title>Conditional Probability</title>

    <p>
      Sometimes events can interact with each other.
      We would like to have the language to talk about a scenario in which evidence that one event has occurred can alter our understanding of the probability of another event occurring.
      We would also like to develop the mathematical tools to quantify exactly how much that probability changes.
    </p>

    <definition xml:id="def-conditional-probability">
      <statement>
        <p>
          Let <m>A</m> and <m>B</m> be events.
          The <term>conditional probability</term> of <m>A</m> given <m>B</m> is
          <md>
            <mrow> \Pr(A \mid B) = \frac{\Pr(A\cap B)}{\Pr(B)}.</mrow>
          </md>
        </p>
      </statement>
    </definition>

    <example xml:id="example-rolls-conditional">
      <statement>
        <p>
          An experiment consists of rolling a fair 6-sided die two times.
          Let <m>A</m> be the event that the sum of the rolls is at least 10.
          To find <m>\Pr(A)</m>, we note that <m>A = \{(4, 6), (5, 5), (5, 6), (6, 4), (6, 5), (6, 6)\}</m>, which has 6 elements.
          Since <m>|\Omega| = 36</m>, we have
          <md>
            <mrow>\Pr(A) = \frac{|A|}{|\Omega|} = \frac{6}{36} = \frac{1}{6}</mrow>
          </md>.
        </p>

        <p>
          Let <m>B</m> be the event that the first roll is 6.
          If we pause after the first die roll seeing the value of 6, we might be more inclined to expect a sum of at least 10.
          The evidence that we've already seen changes our understanding of the situation.
          Since <m>|B| = 6</m>, <m>\Pr(B) = \frac{6}{36} = \frac{1}{6}</m>.
          Also, <m>A\cap B = \{(6, 4), (6, 5), (6, 6)\}</m>, so <m>\Pr(A\cap B) = \frac{3}{36} = \frac{1}{12}</m>.
          Finally:
          <md>
            <mrow>\Pr(A \mid B) = \frac{\Pr(A\cap B)}{\Pr(B)} = \frac{1/12}{1/6} = \frac{1}{2}</mrow>
          </md>
          Before the experiment, we would have said there was only a <m>\frac{1}{6}</m> chance that the sum of the rolls is at least 10.
          However, with the additional knowledge of seeing the first roll of 6, we find the probability of a sum of at least 10 to be <m>\frac{1}{2}</m>, substantially more likely than before.
        </p>
      </statement>
    </example>
  </subsection>


  <subsection xml:id="subsec-diagnostic-testing">
    <title>Diagnostic Testing</title>

    <p>
      Diagnostic tests for diseases aren't perfect.
      When a test comes back positive or negative, a patient will want to understand the (conditional) probability that they have or don't have the disease based on the evidence (the diagnostic test result).
    </p>

    <definition xml:id="def-sensitivity-specificity">
      <statement>
        <p>
          The <term>sensitivity</term> of a diagnostic test is the probability that a patient who has the disease will see a positive test result.
          The <term>specificity</term> is the probability that a patient who does not have the disease will see a negative test result.
        </p>
      </statement>
    </definition>

    <p>
      Introducing event notation, let <m>D</m> be the event that a patient has the disease, and let <m>P</m> be the event that they receive a positive test result.
      Then the sensitivity of the diagnostic test is <m>\Pr(P \mid D)</m>, and the specificity is <m>\Pr(P^c \mid D^c)</m>.
      However, when the patient takes a diagnostic test, the conditional probabilities they would be most interested in would be <m>\Pr(D \mid P)</m> and <m>\Pr(D^c \mid P^c)</m>.
    </p>

    <p>
      Bayes' Theorem expresses the relationship between a conditional probability <m>\Pr(A \mid B)</m> and the flipped conditional probability <m>\Pr(B \mid A)</m>.
    </p>

    <theorem xml:id="thm-Bayes-v1">
      <title>Bayes' Theorem (v1)</title>

      <statement>
        <p>
          <md>
            <mrow> \Pr(B \mid A) = \frac{\Pr(A \mid B)\Pr(B)}{\Pr(A)} </mrow>
          </md>
        </p>
      </statement>
    </theorem>

    <p>
      For example, if a patient sees a positive diagnostic test result, they might try to calculate:
      <md>
        <mrow> \Pr(D \mid P) = \frac{\Pr(P \mid D)\Pr(D)}{\Pr(P)} </mrow>
      </md>
      It will take some work to be able to use this formula.
      We likely don't have direct access to <m>\Pr(P)</m> or <m>\Pr(D)</m>.
    </p>

    <p>
      Let's consider <m>\Pr(P)</m>, the probability of receiving a positive test result.
      The sensitivity <m>\Pr(P \mid D)</m> tells us this probability under the condition that the patient has the disease.
      For a patient who doesn't have the disease, the specificity isn't quite the number we're looking for.
      However, consider the complementary probability:
      <md>
        <mrow> \Pr(P \mid D^c) = 1 - \Pr(P^c \mid D^c) </mrow>
      </md>
      The total <m>\Pr(P)</m> can be divided into two categories: patients who have the disease and test positive, and patients who don't have the disease and test positive.
      So:
      <md>
        <mrow> \Pr(P) \amp = \Pr(P\cap D) + \Pr(P\cap D^c) </mrow>
        <mrow> \amp = \Pr(P\mid D)\Pr(D) + \Pr(P \mid D^c)\Pr(D^c) </mrow>
        <mrow> \amp = \Pr(P\mid D)\Pr(D) + (1 - \Pr(P^c \mid D^c))\Pr(D^c) </mrow>
        <mrow> \amp = (\text{sensitivity})\Pr(D) + (1 - \text{ specificity})\Pr(D^c) </mrow>
      </md>
      We can take this breakdown of <m>\Pr(P)</m> and write a new version of Bayes' Theorem:
    </p>

    <theorem xml:id="thm-Bayes-v2">
      <title>Bayes' Theorem (v2)</title>

      <statement>
        <p>
          <md>
            <mrow> \Pr(B \mid A) = \frac{\Pr(A \mid B)\Pr(B)}{\Pr(A \mid B)\Pr(B) + (1 - \Pr(A^c \mid B^c))\Pr(B^c)} </mrow>
          </md>
        </p>
      </statement>
    </theorem>

    <p>
      We're still missing a crucial piece of information: <m>\Pr(D)</m>, the probability (not conditioned on any evidence) that the patient has the disease.
      This is often referred to as the <term>prior</term>, as in, our prior understanding of the probability of something before we gained some new information from evidence.
      The conditional probability calculated using Bayes' Theorem is usually called the <term>posterior</term> (i.e., after taking evidence into account).
    </p>

    <p>
      There isn't always one single number that's reasonable to use as the prior probability.
      For example, in a diagnostic testing situation, the <term>prevalence</term> of the disease<mdash/>i.e., the proportion of the population who have the disease<mdash/>might feel like a natural number to use as the prior.
      However, what prevalence should you use? During the COVID-19 pandemic, the prevalence of COVID in a particular country, state, and city might be different.
      There's also the possibility of applying Bayes' Theorem multiple times to take into account multiple pieces of evidence, using the posterior probability from one application of Bayes' Theorem to play the role of the prior probability in the next.
      This idea would apply if, for example, a patient took a second diagnostic test to double-check.
      (Complicating the issue further, the developers of diagnostic tests often publish two or even more sets of sensitivity and specificity values, depending on whether a patient is already showing certain symptoms.)
    </p>

    <example>
      <statement>
        <p>
          A 50-year old woman with no symptoms is screened for breast cancer and tests positive.
          If the prevalence of breast cancer for women in her age group is 1% and the particular screening process used has a sensitivity of 90% and a specificity of 91%, what is the probability that the woman has breast cancer given her positive result?
        </p>
      </statement>

      <solution>
        <p>
          Let <m>P</m> be the event of testing positive and <m>D</m> the event of having the disease.
          Then the prevalence <m>\Pr(D)</m> is given as 1%, or 0.01.
          The sensitivity is <m>\Pr(P\mid D) = 0.9</m>, and the specificity is <m>\Pr(P^c\mid D^c) = 0.91</m>.
          So, according to Bayes' Theorem:
          <md>
            <mrow> \Pr(D\mid P) \amp = \frac{\Pr(P\mid D)\Pr(D)}{\Pr(P\mid D)\Pr(D) + (1 - \Pr(P^c\mid D^c))\Pr(D^c)} </mrow>
            <mrow>  \amp = \frac{(0.9)(0.01)}{(0.9)(0.01) + (1 - 0.91)(1 - 0.01)} </mrow>
            <mrow>  \amp \approx 0.092 </mrow>
          </md>
          This may seem like a surprising result.
          Despite sensitivity and specificty values around 90%, it turns out a positive test result only indicates a less than 10% chance of actually having the disease.
          Keep in mind that many ideas in probability and statistics can be highly counterintuitive.
          It's important to be very precise with statements and calculations.
        </p>
      </solution>
    </example>
  </subsection>

  <exercises xml:id="exercises-Conditional-Probability"> <exercisegroup>
  <introduction>
    <p>
      In each of the following scenarios with given events <m>A</m> and <m>B</m>, alculate <m>\Pr(A), \Pr(B)</m>, <m>\Pr(A\cap B)</m>, <m>\Pr(A \mid B)</m>, and <m>\Pr(B \mid A)</m>.
    </p>
  </introduction>

  <exercise>
    <statement>
      <p>
        An experiment consists of rolling a fair die two times.
        Let <m>A</m> be the event that the sum is even, and let <m>B</m> be the event that the second roll is higher than the first.
      </p>
    </statement>

    <answer>
      <p>
        <md>
          <mrow> A = \{ \amp (1, 1), (1, 3), (1, 5), (2, 2), (2, 4), (2, 6), </mrow>
          <mrow> \amp (3, 1), (3, 3), (3, 5), (4, 2), (4, 4), (4, 6), </mrow>
          <mrow> \amp (5, 1), (5, 3), (5, 5), (6, 2), (6, 4), (6, 6)\} </mrow>
          <mrow> B = \{ \amp (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), </mrow>
          <mrow> \amp (2, 3), (2, 4), (2, 5), (2, 6), </mrow>
          <mrow> \amp (3, 4), (3, 5), (3, 6), </mrow>
          <mrow> \amp (4, 5), (4, 6), </mrow>
          <mrow> \amp (5, 6)\} </mrow>
          <mrow> A \cap B = \{ \amp (1, 3), (1, 5), (2, 4), (2, 6), (3, 5), (4, 6)\} </mrow>
        </md>
        So <m>\Pr(A) = \frac{18}{36} = \frac{1}{2}</m>, <m>\Pr(B) = \frac{15}{36} = \frac{5}{12}</m>, and <m>\Pr(A\cap B) = \frac{6}{36} = \frac{1}{6}</m>.
        Finally:
        <md>
          <mrow> \Pr(A \mid B) \amp = \frac{\Pr(A\cap B)}{\Pr(B)} = \frac{6/36}{15/36} = \frac{6}{15} = \frac{2}{5} </mrow>
          <mrow> \Pr(B \mid A) \amp = \frac{\Pr(B\cap A)}{\Pr(A)} = \frac{6/36}{18/36} = \frac{6}{18} = \frac{1}{3} </mrow>
        </md>
        
      </p>
    </answer>
  </exercise>

  <exercise>
    <statement>
      <p>
        An experiment consists of flipping a fair coin three times.
        Let <m>A</m> be the event that the first and second flips match.
        Let <m>B</m> be the event that there are at least two heads.
      </p>
    </statement>

    <answer>
      <p>
        <md>
          <mrow> A \amp = \{ HHH, HHT, TTH, TTT \} </mrow>
          <mrow> B \amp = \{ HHH, HHT, HTH, THH \} </mrow>
          <mrow> A\cap B \amp = \{HHH, HHT\} </mrow>
        </md>
        So <m>\Pr(A) = \frac{4}{8} = \frac{1}{2}</m>, <m>\Pr(B) = \frac{4}{8} = \frac{1}{2}</m>, and <m>\Pr(A\cap B) = \frac{2}{8} = \frac{1}{4}</m>.
        Finally:
        <md>
          <mrow> \Pr(A \mid B) \amp = \frac{\Pr(A\cap B)}{\Pr(B)} = \frac{2/8}{4/8} = \frac{2}{4} = \frac{1}{2} </mrow>
          <mrow> \Pr(B \mid A) \amp = \frac{\Pr(B\cap A)}{\Pr(A)} = \frac{2/8}{4/8} = \frac{2}{4} = \frac{1}{2} </mrow>
        </md>
      </p>
    </answer>
  </exercise>
  </exercisegroup>

  <exercise>
    <introduction>
      <p>
        A diagnostic test is developed to detect a disease present in 3.2% of the population.
        For a patient who has the disease, the test will accurately give a positive result 65% of the time.
        When the patient does not have the disease, the test will accurately give a negative result 99.9% of the time.
      </p>
    </introduction>


    <task>
      <statement>
        <p>
          For a patient who receives a positive test, what is the probability they have the disease?
        </p>
      </statement>

      <answer>
        <p>
          Let <m>P</m> be the event of testing positive and <m>D</m> the event of having the disease.
          Then the prevalence <m>\Pr(D)</m> is given as 3.2%, or 0.032.
          The sensitivity is <m>\Pr(P\mid D) = 0.65</m>, and the specificity is <m>\Pr(P^c\mid D^c) = 0.999</m>.
          So, according to Bayes' Theorem:
          <md>
            <mrow> \Pr(D\mid P) \amp = \frac{\Pr(P\mid D)\Pr(D)}{\Pr(P\mid D)\Pr(D) + (1 - \Pr(P^c\mid D^c))\Pr(D^c)} </mrow>
            <mrow>  \amp = \frac{(0.65)(0.032)}{(0.65)(0.032) + (1 - 0.999)(1 - 0.032)} </mrow>
            <mrow>  \amp \approx 0.96 </mrow>
          </md>
        </p>
      </answer>
    </task>


    <task>
      <statement>
        <p>
          For a patient who receives a negative test, what is the probability they do not have the disease?
        </p>
      </statement>

      <answer>
        <p>
          <md>
            <mrow> \Pr(D^c\mid P^c) \amp = \frac{\Pr(P^c\mid D^c)\Pr(D^c)}{\Pr(P^c\mid D^c)\Pr(D^c) + (1 - \Pr(P\mid D))\Pr(D)} </mrow>
            <mrow>  \amp = \frac{(0.999)(1 - 0.032)}{(0.999)(1 - 0.032) + (1 - 0.65)(0.032)} </mrow>
            <mrow>  \amp \approx 0.99 </mrow>
          </md>
        </p>
      </answer>
    </task>
  </exercise>
  </exercises>
</section>